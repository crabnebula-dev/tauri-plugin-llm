[package]
name = "tauri-plugin-tauri-plugin-llm"
version = "0.1.0"
authors = ["You"]
description = ""
edition = "2021"
rust-version = "1.77.2"
exclude = ["/examples", "/dist-js", "/guest-js", "/node_modules"]
links = "tauri-plugin-tauri-plugin-llm"

[dependencies]
tauri = { version = "2.9.4" }
serde = "1.0"
thiserror = "2"
mcpurify = { path = "../mcpurify" }
anyhow              = {version = "1.0"}
serde_json          = {version = "1.0"}

# for LLM inference
candle-core         = { git = "https://github.com/huggingface/candle.git", features = ["metal"] }
candle-nn           = { git = "https://github.com/huggingface/candle.git", features = ["metal"] }
candle-transformers = { git = "https://github.com/huggingface/candle.git", features = ["metal"] }
tokenizers          = { git = "https://github.com/huggingface/tokenizers", branch = "main" }

[build-dependencies]
tauri-plugin = { version = "2.5.2", features = ["build"] }

[features]
default = ["feat_safetensors"]
feat_safetensors = []