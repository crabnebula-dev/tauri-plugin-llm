[package]
name = "tauri-plugin-llm"
version = "0.1.0"
authors = [
    "Matthias Kandora<matthias@crabnebula.dev>",
    "Fabian-Lars Scheidt<fabianlars@crabnebula.dev>",
]
description = "Load LLMs and makes them accessible as Tauri plugin"
edition = "2021"
rust-version = "1.77.2"
exclude = ["/examples", "/dist-js", "/guest-js", "/node_modules"]
links = "tauri-plugin-llm"

[workspace]
members = ["examples/tauri-app/src-tauri"]

[dependencies]
tauri = { version = "2.9.4" }
serde = "1"
serde_json = "1"
thiserror = "2"
mcpurify = { path = "../mcpurify", optional = true }
anyhow = "1"
tokio = { version = "1.48", features = ["full"] }
rand = "0.9"
failsafe = "1"

# for LLM inference
candle-core = { git = "https://github.com/huggingface/candle.git", features = [
    "metal",
] }
candle-nn = { git = "https://github.com/huggingface/candle.git", features = [
    "metal",
] }
candle-transformers = { git = "https://github.com/huggingface/candle.git", features = [
    "metal",
] }
tokenizers = { git = "https://github.com/huggingface/tokenizers", branch = "main" }

# for tracing
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["json"] }
tracing-appender = "0.2"

[build-dependencies]
tauri-plugin = { version = "2.5.2", features = ["build"] }

[dev-dependencies]
proptest = "1"

[features]
default = ["feat_safetensors", "apple_metal"]
feat_safetensors = []
cuda = []
apple_metal = []
mcpurify = ["dep:mcpurify"]
