[package]
name = "tauri-plugin-llm"
version = "0.1.0"
authors = ["You"]
description = ""
edition = "2021"
rust-version = "1.77.2"
exclude = ["/examples", "/dist-js", "/guest-js", "/node_modules"]
links = "tauri-plugin-llm"

[dependencies]
tauri = { version = "2.9.4" }
serde = "1.0"
thiserror = "2"
mcpurify = { path = "../mcpurify" }
anyhow              = {version = "1.0"}
serde_json          = {version = "1.0"}
tokio               = {version = "1.48.0", features = ["full"]}

# for LLM inference
candle-core         = { git = "https://github.com/huggingface/candle.git", features = ["metal"] }
candle-nn           = { git = "https://github.com/huggingface/candle.git", features = ["metal"] }
candle-transformers = { git = "https://github.com/huggingface/candle.git", features = ["metal"] }
tokenizers          = { git = "https://github.com/huggingface/tokenizers", branch = "main" }

# for tracing 
tracing             = {version = "0.1"}
tracing-subscriber  = {version = "0.3.20", features = ["json"]}
tracing-appender    = {version = "0.2.3"}

[build-dependencies]
tauri-plugin = { version = "2.5.2", features = ["build"] }

[dev-dependencies]
proptest            = { version = "1.9.0" }

[features]
default = ["feat_safetensors", "apple_metal"]
feat_safetensors = []
cuda    = []
apple_metal   = []