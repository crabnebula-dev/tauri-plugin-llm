version: '3'

env:
  # global env setting to eny resolving large file pointers to actual blobs
  GIT_LFS_SKIP_SMUDGE: 1
  CONTAINER_RUNTIME: container

tasks:
  default:
    silent: true
    desc: List all available tasks 
    cmds:
      - task --list-all

  reset:
    silent: true
    desc: Resets all large files to git lfs/xet file pointers to reclaim disk space
    dir: models/
    cmds:
      - find . -maxdepth 1 -mindepth 1 -type d -exec bash -c 'cd $0 && git read-tree HEAD && git checkout -f HEAD' {} \;

  container:
    silent: true
    desc: Runs a container and mounts the current directory into /code
    cmds:
      - $CONTAINER_RUNTIME run -ti -v $(pwd):/code --cpus 8 -m 16G development

  # ensure 'models' dir exists
  model_dir:
    desc: Create the directory 'model', if it does not exist
    silent: true
    cmds: 
      - mkdir models
    status:
      - test -d models

  # qwen3 4b instruct base model for configuration etc
  fetch_Qwen3_4B_Instruct_2507_FP8:
    deps: [model_dir]
    silent: true
    desc: Fetches Qwen3-4B-Instruct-2507-FP8 if  not available. This directory contains the `config.json`, `tokenizer.json`
    dir: models/
    cmds: 
      - git clone https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507-FP8
      - cd Qwen3-4B-Instruct-2507-FP8 && git lfs pull --inlude "model.safetensors"
    status:
      - test -d Qwen3-4B-Instruct-2507-FP8

  # Qwen3-4B Repo with all model weights
  fetch_Qwen3_4B_GGUF:
    deps: [model_dir, fetch_Qwen3_4B_Instruct_2507_FP8]
    silent: true
    dir: models
    cmds:
      - git clone https://huggingface.co/unsloth/Qwen3-4B-GGUF
    status:
      - test -d Qwen3-4B-GGUF

  # Install individual Qwen3 tokenizer.json
  fetch_Qwen3_4B_Instruct_2507_FP8_tokenizer.json:
    deps: [model_dir, fetch_Qwen3_4B_Instruct_2507_FP8]
    silent: true
    desc: Fetches Qwen3_4B_Instruct_2507_FP8 tokenizer.json
    dir: models/Qwen3-4B-Instruct-2507-FP8
    cmds:
      - git lfs pull --include "tokenizer.json"
    status:
      - bash -c 'git lfs pointer --check --file Qwen3_4B_Instruct_2507_FP8/tokenizer.json'

  # Install individual Qwen3 model weight
  fetch_Qwen3_4B_K_M_GGUF:
    deps: [model_dir, fetch_Qwen3_4B_GGUF, fetch_Qwen3_4B_Instruct_2507_FP8]
    silent: true
    desc: Fetches Qwen3-4B-GGUF_M_M model weights
    dir: models/Qwen3-4B-GGUF
    cmds:
      - git lfs pull --include "Qwen3-4B-Q4_K_M.gguf"
    status:
      - bash -c 'git lfs pointer --check --file Qwen3-4B-GGUF/Qwen3-4B-Q4_K_M.gguf'

  # fetch llama 3.2 3B instruct
  fetch_Llama_3.2_3B_Instruct:
    silent: true
    desc: Fetches Llama_3.2_3B_Instruct
    dir: models/
    cmds:
      - git clone https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
    status:
      - test -d Llama-3.2-3B-Instruct

  fetch_gemma_3_27b_it_macos:
    silent: true
    platforms: [darwin]
    desc: Fetches gemma-3-27b-it
    dir: models/
    env:  
      HF_TOKEN:
        sh: security find-generic-password -s "huggingface-token" -w
    cmds:
      - git clone https://x:$HF_TOKEN@huggingface.co/google/gemma-3-27b-it
    status:
      - test -d gemma-3-27b-it

  fetch_gemma_3_27b_it_safetensors:
    deps: [fetch_gemma_3_27b_it_macos]
    silent: true
    desc: Fetch all safetensors parts
    dir: models/gemma-3-27b-it
    cmds:
      - git lfs pull --include "*.safetensors"

